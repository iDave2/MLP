{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "Hello, Jupyter.\n",
    "\n",
    "## v0.1 Data\n",
    "\n",
    "This initial commit was about getting the data to the browser as a sequence of `(image, label)` pairs which I am calling `elements`.  So big review of nodejs, especially Streams and Promises.  Result is a presentation of elements that may be scrolled or, since no-one wants to scroll 60,000 elements, to jump to a given element index in the MNIST database.  This provides some confidence that we are accessing the correct data.  With streams, it also uses less than 13KB in browser (if a *minimize memory* option is used) although node may hold a 64KB file buffer underneath.\n",
    "\n",
    "<img src=\"web/images/scrollbar.png\" alt=\"Scrollbar\" style=\"width: 527px;\"/>\n",
    "\n",
    "## v0.2 Next\n",
    "\n",
    "Fumbling through Chapter 1 of [Sir Nielsen's Book](http://neuralnetworksanddeeplearning.com/chap1.html), there is an idea I'd like to try before diving into stochastic gradient descent.  While I rail against premature optimization elsewhere, it just seems like we can get off the starting block quicker having foreknowledge of categories, the digits $[0,9]$ in this case.\n",
    "\n",
    "Consider an image abstractly as a looong vector of 784 numbers.  Without foreknowledge, it is just a bunch of meaningless numbers.  With foreknowledge, we know that it belongs to one of the categories and we know which category each looong vector belongs to.  What if we do this,\n",
    "\n",
    "- Create ten neurons, the output neurons for each digit;\n",
    "- Connect each output neuron to all 784 input neurons assigning a weight of $1.0$ to each connection;\n",
    "- Attach a separate $784\\times1$ vector initialized to zeros to each output neuron;\n",
    "- Run all the training data through this system.\n",
    "\n",
    "Then for each incoming element, for each incoming $(vector, label)$, each output neuron sees the vector itself since all weights are '1' and we can,\n",
    "\n",
    "- Add incoming vector to the attached vector on the output neuron whose category matches the incoming label;\n",
    "- When all training elements have passed through, divide the vector attached to *each* output neuron by the number of elements scanned (or just make each attached vector have length 1).\n",
    "\n",
    "This provides an average vector, a *best* matching image, for each digit.  Maybe this forms a $10$-dimensional basis for the projection of incoming $[784\\times1]$ vectors onto the $[10\\times1]$ output space.  Now run the test elements through the system and for each incoming $(vector, label)$,\n",
    "\n",
    "- Take the dot product of incoming $vector$ with *each* output neuron's average vector;\n",
    "- The size of this scalar is proportional to the confidence that incoming $vector$ corresponds to that output neuron's digit;\n",
    "\n",
    "Does this provide clean winners?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
